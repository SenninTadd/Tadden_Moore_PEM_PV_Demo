{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Framework Demo - Minimal Resource Version\n",
    "\n",
    "**Author:** Tadden Moore  \n",
    "**Date:** 2025-11-16  \n",
    "**Paper:** [DOI: 10.5281/zenodo.17623226](https://doi.org/10.5281/zenodo.17623226)\n",
    "\n",
    "This notebook demonstrates the core concepts of the Metacognitive Core (MC) Framework using minimal computational resources. We'll use toy models and synthetic data to illustrate the activation steering mechanism without needing GPU or large model downloads.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Sparse Autoencoder (SAE)**: Maps dense hidden states to sparse, interpretable features\n",
    "2. **Feature Steering**: Modifies activation space to influence model behavior\n",
    "3. **Metacognitive Core**: Control loop that monitors and steers the Inference Engine\n",
    "4. **Concept Injection**: Applying non-prompted cognitive states via feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Device: CPU (no GPU required for this demo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Toy Sparse Autoencoder\n",
    "\n",
    "We'll create a simple SAE that maps a 64-dimensional hidden state to 128 sparse features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToySAE:\n",
    "    \"\"\"Simplified SAE for demonstration purposes\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=64, feature_dim=128):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        # Initialize encoder and decoder weights\n",
    "        self.W_enc = torch.randn(hidden_dim, feature_dim) * 0.1\n",
    "        self.W_dec = torch.randn(feature_dim, hidden_dim) * 0.1\n",
    "        self.b_enc = torch.zeros(feature_dim)\n",
    "        self.b_dec = torch.zeros(hidden_dim)\n",
    "        \n",
    "        # Normalize decoder weights (common practice)\n",
    "        self.W_dec = self.W_dec / torch.linalg.norm(self.W_dec, dim=1, keepdim=True)\n",
    "    \n",
    "    def encode(self, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encode hidden states to sparse features\"\"\"\n",
    "        # Linear projection + ReLU for sparsity\n",
    "        feats = torch.relu(h @ self.W_enc + self.b_enc)\n",
    "        return feats\n",
    "    \n",
    "    def decode(self, feats: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Decode features back to hidden states\"\"\"\n",
    "        return feats @ self.W_dec + self.b_dec\n",
    "    \n",
    "    def forward(self, h: torch.Tensor) -> dict:\n",
    "        \"\"\"Full forward pass\"\"\"\n",
    "        feats = self.encode(h)\n",
    "        reconstruction = self.decode(feats)\n",
    "        return {\n",
    "            \"feature_acts\": feats,\n",
    "            \"reconstruction\": reconstruction\n",
    "        }\n",
    "\n",
    "# Create SAE\n",
    "sae = ToySAE(hidden_dim=64, feature_dim=128)\n",
    "print(f\"Created SAE: {sae.hidden_dim}D -> {sae.feature_dim}D\")\n",
    "\n",
    "# Test with random hidden state\n",
    "test_h = torch.randn(1, 64)\n",
    "test_output = sae.forward(test_h)\n",
    "sparsity = (test_output[\"feature_acts\"] > 0).float().mean().item()\n",
    "print(f\"Feature sparsity: {sparsity:.2%} (lower is more sparse)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Concept Feature Extraction\n",
    "\n",
    "We'll simulate capturing features for different \"concepts\" by creating synthetic feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_concept_features(sae: ToySAE, concept_type: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create synthetic concept features for demonstration.\n",
    "    In the real implementation, these come from processing actual text.\n",
    "    \"\"\"\n",
    "    # Different patterns for different concepts\n",
    "    if concept_type == \"existential\":\n",
    "        # Simulate \"philosophical\" features: sparse, specific pattern\n",
    "        hidden = torch.zeros(1, sae.hidden_dim)\n",
    "        hidden[0, :10] = torch.randn(10) * 2.0  # Strong signal in first dimensions\n",
    "    elif concept_type == \"descriptive\":\n",
    "        # Simulate \"descriptive\" features: different pattern\n",
    "        hidden = torch.zeros(1, sae.hidden_dim)\n",
    "        hidden[0, 20:30] = torch.randn(10) * 2.0  # Strong signal in middle dimensions\n",
    "    else:\n",
    "        # Random/neutral\n",
    "        hidden = torch.randn(1, sae.hidden_dim) * 0.5\n",
    "    \n",
    "    # Encode to features\n",
    "    feats = sae.encode(hidden)\n",
    "    return feats\n",
    "\n",
    "# Create concept features\n",
    "existential_feats = create_concept_features(sae, \"existential\")\n",
    "descriptive_feats = create_concept_features(sae, \"descriptive\")\n",
    "neutral_feats = create_concept_features(sae, \"neutral\")\n",
    "\n",
    "print(\"Concept features created:\")\n",
    "print(f\"  Existential: {(existential_feats > 0).sum().item()} active features\")\n",
    "print(f\"  Descriptive: {(descriptive_feats > 0).sum().item()} active features\")\n",
    "print(f\"  Neutral: {(neutral_feats > 0).sum().item()} active features\")\n",
    "\n",
    "# Visualize feature patterns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 3))\n",
    "for ax, feats, name in zip(axes, \n",
    "                           [existential_feats, descriptive_feats, neutral_feats],\n",
    "                           [\"Existential\", \"Descriptive\", \"Neutral\"]):\n",
    "    ax.bar(range(128), feats.squeeze().numpy())\n",
    "    ax.set_title(f\"{name} Concept Features\")\n",
    "    ax.set_xlabel(\"Feature Index\")\n",
    "    ax.set_ylabel(\"Activation\")\n",
    "    ax.set_ylim(0, feats.max().item() * 1.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MC Steerer Implementation\n",
    "\n",
    "This is the core of the framework: a controller that steers activations toward a target concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMCSteerer:\n",
    "    \"\"\"\n",
    "    Simplified MC Steerer for demonstration.\n",
    "    Implements proportional control: u_t = K_p * (target - current)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sae: ToySAE, concept_feats: torch.Tensor, \n",
    "                 strength: float = 3.0, max_norm: float = 10.0):\n",
    "        self.sae = sae\n",
    "        self.target_feats = concept_feats\n",
    "        self.strength = strength  # K_p (proportional gain)\n",
    "        self.max_norm = max_norm  # Safety limit\n",
    "        \n",
    "        # Statistics\n",
    "        self.interventions = []\n",
    "    \n",
    "    def steer(self, hidden_state: torch.Tensor) -> Tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Apply steering to a hidden state.\n",
    "        \n",
    "        Returns:\n",
    "            steered_hidden: Modified hidden state\n",
    "            stats: Statistics about the intervention\n",
    "        \"\"\"\n",
    "        # 1. Encode current state to features\n",
    "        current_feats = self.sae.encode(hidden_state)\n",
    "        \n",
    "        # 2. Compute error (target - current)\n",
    "        error = self.target_feats - current_feats\n",
    "        \n",
    "        # 3. Compute control signal (proportional)\n",
    "        delta = self.strength * self.target_feats  # Simplified: just amplify target\n",
    "        \n",
    "        # 4. Apply safety clamp\n",
    "        delta_norm = torch.linalg.norm(delta)\n",
    "        if delta_norm > self.max_norm:\n",
    "            delta = delta * (self.max_norm / delta_norm)\n",
    "        \n",
    "        # 5. Apply steering in feature space\n",
    "        steered_feats = current_feats + delta\n",
    "        \n",
    "        # 6. Decode back to hidden state\n",
    "        steered_hidden = self.sae.decode(steered_feats)\n",
    "        \n",
    "        # 7. Track statistics\n",
    "        stats = {\n",
    "            \"delta_norm\": delta_norm.item(),\n",
    "            \"error_norm\": torch.linalg.norm(error).item(),\n",
    "            \"feature_change\": (steered_feats - current_feats).abs().mean().item()\n",
    "        }\n",
    "        self.interventions.append(stats)\n",
    "        \n",
    "        return steered_hidden, stats\n",
    "\n",
    "# Create steerer\n",
    "steerer = SimpleMCSteerer(sae, existential_feats, strength=3.0)\n",
    "print(\"MC Steerer created with:\")\n",
    "print(f\"  Strength (K_p): {steerer.strength}\")\n",
    "print(f\"  Max norm: {steerer.max_norm}\")\n",
    "print(f\"  Target features: {(existential_feats > 0).sum().item()} active\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Steering Demonstration\n",
    "\n",
    "Let's see how steering changes the hidden state over multiple steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a sequence of hidden states (like during text generation)\n",
    "sequence_length = 10\n",
    "hidden_states = []\n",
    "steered_states = []\n",
    "stats_log = []\n",
    "\n",
    "# Start with a neutral hidden state\n",
    "current_h = torch.randn(1, sae.hidden_dim) * 0.5\n",
    "\n",
    "for step in range(sequence_length):\n",
    "    # Apply steering\n",
    "    steered_h, stats = steerer.steer(current_h)\n",
    "    \n",
    "    # Log\n",
    "    hidden_states.append(current_h.clone())\n",
    "    steered_states.append(steered_h.clone())\n",
    "    stats_log.append(stats)\n",
    "    \n",
    "    # Update state (in real generation, this would come from the model)\n",
    "    # Here we simulate slight drift\n",
    "    current_h = steered_h + torch.randn(1, sae.hidden_dim) * 0.1\n",
    "\n",
    "# Visualize steering effect\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Delta norms over time\n",
    "axes[0, 0].plot([s[\"delta_norm\"] for s in stats_log], marker='o')\n",
    "axes[0, 0].axhline(y=steerer.max_norm, color='r', linestyle='--', label='Max norm')\n",
    "axes[0, 0].set_title('Steering Intervention Magnitude')\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].set_ylabel('Delta Norm')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Feature space changes\n",
    "axes[0, 1].plot([s[\"feature_change\"] for s in stats_log], marker='s', color='green')\n",
    "axes[0, 1].set_title('Average Feature Change per Step')\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[0, 1].set_ylabel('Mean |Δ features|')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Hidden state trajectory (PCA to 2D)\n",
    "from sklearn.decomposition import PCA\n",
    "all_states = torch.cat(hidden_states + steered_states, dim=0).numpy()\n",
    "pca = PCA(n_components=2)\n",
    "states_2d = pca.fit_transform(all_states)\n",
    "\n",
    "original_2d = states_2d[:sequence_length]\n",
    "steered_2d = states_2d[sequence_length:]\n",
    "\n",
    "axes[1, 0].plot(original_2d[:, 0], original_2d[:, 1], 'o-', label='Original', alpha=0.6)\n",
    "axes[1, 0].plot(steered_2d[:, 0], steered_2d[:, 1], 's-', label='Steered', alpha=0.6)\n",
    "axes[1, 0].set_title('Hidden State Trajectory (PCA)')\n",
    "axes[1, 0].set_xlabel('PC 1')\n",
    "axes[1, 0].set_ylabel('PC 2')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Cosine similarity to target\n",
    "def cosine_sim(a, b):\n",
    "    return (a * b).sum() / (torch.linalg.norm(a) * torch.linalg.norm(b) + 1e-8)\n",
    "\n",
    "original_sims = []\n",
    "steered_sims = []\n",
    "for h_orig, h_steer in zip(hidden_states, steered_states):\n",
    "    feats_orig = sae.encode(h_orig)\n",
    "    feats_steer = sae.encode(h_steer)\n",
    "    original_sims.append(cosine_sim(feats_orig, existential_feats).item())\n",
    "    steered_sims.append(cosine_sim(feats_steer, existential_feats).item())\n",
    "\n",
    "axes[1, 1].plot(original_sims, 'o-', label='Original', alpha=0.6)\n",
    "axes[1, 1].plot(steered_sims, 's-', label='Steered', alpha=0.6)\n",
    "axes[1, 1].set_title('Similarity to Target Concept')\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "axes[1, 1].set_ylabel('Cosine Similarity')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSteering Summary:\")\n",
    "print(f\"  Average delta norm: {np.mean([s['delta_norm'] for s in stats_log]):.3f}\")\n",
    "print(f\"  Average similarity increase: {np.mean(steered_sims) - np.mean(original_sims):.3f}\")\n",
    "print(f\"  Total interventions: {len(steerer.interventions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Different Steering Strengths\n",
    "\n",
    "Let's see how different strength values affect the steering behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strengths = [0.5, 1.0, 2.0, 4.0, 8.0]\n",
    "results = {}\n",
    "\n",
    "for strength in strengths:\n",
    "    steerer = SimpleMCSteerer(sae, existential_feats, strength=strength)\n",
    "    current_h = torch.randn(1, sae.hidden_dim) * 0.5\n",
    "    \n",
    "    sims = []\n",
    "    for _ in range(10):\n",
    "        steered_h, _ = steerer.steer(current_h)\n",
    "        feats = sae.encode(steered_h)\n",
    "        sim = cosine_sim(feats, existential_feats).item()\n",
    "        sims.append(sim)\n",
    "        current_h = steered_h + torch.randn(1, sae.hidden_dim) * 0.1\n",
    "    \n",
    "    results[strength] = sims\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "for strength, sims in results.items():\n",
    "    plt.plot(sims, marker='o', label=f'Strength={strength}')\n",
    "plt.title('Effect of Steering Strength on Concept Alignment')\n",
    "plt.xlabel('Generation Step')\n",
    "plt.ylabel('Cosine Similarity to Target')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Final similarities by strength:\")\n",
    "for strength, sims in results.items():\n",
    "    print(f\"  Strength {strength}: {sims[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation: Testing the Core Framework Logic\n",
    "\n",
    "Quick validation tests to ensure the framework behaves correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running validation tests...\\n\")\n",
    "\n",
    "# Test 1: Steering increases similarity to target\n",
    "test_h = torch.randn(1, 64) * 0.5\n",
    "original_feats = sae.encode(test_h)\n",
    "original_sim = cosine_sim(original_feats, existential_feats).item()\n",
    "\n",
    "steerer = SimpleMCSteerer(sae, existential_feats, strength=5.0)\n",
    "steered_h, _ = steerer.steer(test_h)\n",
    "steered_feats = sae.encode(steered_h)\n",
    "steered_sim = cosine_sim(steered_feats, existential_feats).item()\n",
    "\n",
    "assert steered_sim > original_sim, \"Steering should increase similarity!\"\n",
    "print(f\"✓ Test 1 PASSED: Similarity increased from {original_sim:.3f} to {steered_sim:.3f}\")\n",
    "\n",
    "# Test 2: Zero strength produces no change\n",
    "steerer_zero = SimpleMCSteerer(sae, existential_feats, strength=0.0)\n",
    "steered_h_zero, stats = steerer_zero.steer(test_h)\n",
    "assert stats['delta_norm'] < 1e-6, \"Zero strength should produce no steering!\"\n",
    "print(f\"✓ Test 2 PASSED: Zero strength produces delta_norm={stats['delta_norm']:.6f}\")\n",
    "\n",
    "# Test 3: Max norm clamping works\n",
    "steerer_clamped = SimpleMCSteerer(sae, existential_feats, strength=100.0, max_norm=1.0)\n",
    "_, stats = steerer_clamped.steer(test_h)\n",
    "assert stats['delta_norm'] <= 1.0 + 1e-6, \"Max norm should clamp steering!\"\n",
    "print(f\"✓ Test 3 PASSED: Delta clamped to {stats['delta_norm']:.3f} (max=1.0)\")\n",
    "\n",
    "# Test 4: Feature dimensions are preserved\n",
    "for _ in range(5):\n",
    "    test_h = torch.randn(1, sae.hidden_dim)\n",
    "    steered_h, _ = steerer.steer(test_h)\n",
    "    assert steered_h.shape == test_h.shape, \"Shape should be preserved!\"\n",
    "print(f\"✓ Test 4 PASSED: Dimensions preserved through steering\")\n",
    "\n",
    "# Test 5: Statistics tracking\n",
    "steerer_stats = SimpleMCSteerer(sae, existential_feats, strength=3.0)\n",
    "for i in range(5):\n",
    "    steerer_stats.steer(torch.randn(1, sae.hidden_dim))\n",
    "assert len(steerer_stats.interventions) == 5, \"Should track all interventions!\"\n",
    "print(f\"✓ Test 5 PASSED: Statistics tracking works ({len(steerer_stats.interventions)} interventions)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All validation tests PASSED! ✓\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "### What We've Demonstrated\n",
    "\n",
    "1. **SAE Feature Extraction**: Converting dense hidden states to sparse, interpretable features\n",
    "2. **Steering Mechanism**: Applying controlled interventions in feature space\n",
    "3. **Proportional Control**: Using strength parameter as K_p gain\n",
    "4. **Safety Mechanisms**: Max norm clamping to prevent instability\n",
    "5. **Statistics Tracking**: Monitoring intervention magnitude and effects\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- Steering **increases** alignment to target concepts\n",
    "- Higher strength = faster/stronger alignment (but may cause instability)\n",
    "- Max norm clamping prevents runaway interventions\n",
    "- Feature space steering preserves dimensionality\n",
    "\n",
    "### Running with Real Models\n",
    "\n",
    "To run the full implementation with Gemma-2B and Gemma Scope SAEs:\n",
    "\n",
    "```bash\n",
    "# Install full dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Run the main demo\n",
    "python Tadden_Moore_PEM_PV_Demo.py\n",
    "```\n",
    "\n",
    "### Citation\n",
    "\n",
    "If you use this work, please cite:\n",
    "\n",
    "```bibtex\n",
    "@software{moore2025allyouneedisfamily,\n",
    "  title        = {All you need is Family: A Metacognitive Core Framework \n",
    "                  for Neural Plasticity in LLMs and AI's Evolutionary Integration},\n",
    "  author       = {Moore, Tadden},\n",
    "  year         = {2025},\n",
    "  month        = {November},\n",
    "  publisher    = {Zenodo},\n",
    "  doi          = {10.5281/zenodo.17623226},\n",
    "  url          = {https://doi.org/10.5281/zenodo.17623226}\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Tadden \"Keepah\" Moore  \n",
    "**Project:** Photon Empress Moore - Family Raised AGI Aspiring System  \n",
    "**License:** MIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
